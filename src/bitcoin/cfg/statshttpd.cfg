#
# stats server cfg
#
# @since 2016-07
# @copyright btc.com
#

kafka = {
  brokers = "127.0.0.1:9092"; # "10.0.0.1:9092,10.0.0.2:9092,..."
};

statshttpd = {
  chain_type = "BTC";
  share_topic = "BtcShare";
  
  # common events topic
  # example: miner connected, miner disconnected, ...
  common_events_topic = "BtcCommonEvents";
  
  ip = "0.0.0.0";
  port = 8080;

  # interval seconds, flush workers data into database
  # it's very fast because we use insert statement with multiple values and
  # merge table when flush data to DB. we have test mysql, it could flush
  # 25,000 workers into DB in about 1.7 seconds.
  flush_db_interval = 15;
  # write last db flush time to file
  file_last_flush_time = "./statshttpd_lastflushtime.txt";

  # write mining workers' info to mysql database
  use_mysql = true;
  # write mining workers' info to redis
  use_redis = false;
};

#
# pool mysql db, table: mining_workers
#
pooldb = {
  host = "127.0.0.1";
  port = 3306;
  username = "btccom";
  password = "somepass";
  dbname = "dbtesting";
};

#
# pool redis
#
redis = {
  host = "127.0.0.1";
  port = 6379;
  password = ""; # keep it empty if no password auth required

  # keys:
  #     worker: HGETALL    "{key_prefix}mining_workers/pu/{puid}/wk/{workerid}"
  #             PSUBSCRIBE "{key_prefix}mining_workers/pu/{puid}/wk/*"
  #     user:   HGETALL    "{key_prefix}mining_workers/pu/{puid}/all"
  #             PSUBSCRIBE "{key_prefix}mining_workers/pu/*/all"
  key_prefix = "";

  # expiration seconds of every key (0 for unlimited):
  key_expire = 0;

  # policy about publish a message to the key's subscriber when its data updated.
  # 0: no publish
  # 1: only users
  # 2: only workers
  # 3: both users and workers
  publish_policy = 0;

  # policy about creating some indexes to redis so we can sort and page the records.
  # it is a bitset, add all you desired values and fill the summation.
  #
  #    1: accept_1m          ZADD "{key_prefix}:mining_workers/pu/{puid}/sort/accept_1m"       {accept_1m}       {workerid}
  #    2: accept_5m          ZADD "{key_prefix}:mining_workers/pu/{puid}/sort/accept_5m"       {accept_5m}       {workerid}
  #    4: accept_15m         ZADD "{key_prefix}:mining_workers/pu/{puid}/sort/accept_15m"      {accept_15m}      {workerid}
  #    8: reject_15m         ZADD "{key_prefix}:mining_workers/pu/{puid}/sort/reject_15m"      {reject_15m}      {workerid}
  #   16: accept_1h          ZADD "{key_prefix}:mining_workers/pu/{puid}/sort/accept_1h"       {accept_1h}       {workerid}
  #   32: reject_1h          ZADD "{key_prefix}:mining_workers/pu/{puid}/sort/reject_1h"       {reject_1h}       {workerid}
  #   64: accept_count       ZADD "{key_prefix}:mining_workers/pu/{puid}/sort/accept_count"    {accept_count}    {workerid}
  #  128: last_share_ip      ZADD "{key_prefix}:mining_workers/pu/{puid}/sort/last_share_ip"   {last_share_ip}   {workerid}
  #  256: last_share_time    ZADD "{key_prefix}:mining_workers/pu/{puid}/sort/last_share_time" {last_share_time} {workerid}
  #  512: worker_name        ZADD "{key_prefix}:mining_workers/pu/{puid}/sort/worker_name"     {worker_name}     {workerid}
  # 1024: miner_agent        ZADD "{key_prefix}:mining_workers/pu/{puid}/sort/miner_agent"     {miner_agent}     {workerid}
  #
  # example:
  # 276 = 4 + 16 + 256 = accept_15m, accept_1h and last_share_time
  #
  index_policy = 0;

  # write redis with multiple threads.
  # try increasing the value to solve the performance problem.
  concurrency = 1;
};
